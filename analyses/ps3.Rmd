---
title: 'Psych 254 W15 PS #3'
author: "Mike Frank"
date: "February 22, 2015"
output: html_document
---

This is problem set #3, in which we want you to integrate your knowledge of data wrangling with some basic simulation skills and some linear modeling.

```{r}
library(dplyr)
library(ggplot2)
```

Part 1: Basic simulation and NHST
=================================

Let's start by convincing ourselves that t-tests have the appropriate false positive rate. Run 10,000 t-tests with standard, normally-distributed data from a made up 30-person, single-measurement experiment (the command for sampling from a normal distribution is `rnorm`). What's the mean number of "significant" results?

First do this using a `for` loop.

```{r}
res_p = c()
counter = 0
for (i in 1:10000) {
  rs = t.test(rnorm(30,0,1))$p.value
  res_p = c(res_p,rs)
  if (rs <= .05) {
    counter = counter + 1
  }
}
counter / 10000
```

Next, do this using the `replicate` function:

```{r}
rs = replicate(10000,t.test(rnorm(30,0,1))$p.value)
count = sum(rs<=.05)
count / 10000
```

Ok, that was a bit boring. Let's try something moderately more interesting - let's implement a p-value sniffing simulation, in the style of Simons, Nelson, & Simonsohn (2011).

Consider this scenario: you have done an experiment, again with 30 participants (one observation each, just for simplicity). The question is whether their performance is above chance. You aren't going to check the p-value every trial, but let's say you run 30 - then if the p-value is within the range p < .25 and p > .05, you optionally run 30 more and add those data, then test again. But if the original p value is < .05, you call it a day, and if the original is > .25, you also stop.  

First, write a function that implements this sampling regime.

```{r}
double.sample <- function (lower,upper) {
  values = rnorm(30,0,1)
  first = t.test(values)$p.value
  if (first > lower && first < upper) {
    values = c(values, rnorm(30,0,1))
    return(t.test(values)$p.value)
  }
  return(first)
}
```

Now call this function 10k times and find out what happens. 

```{r}
rs = replicate(10000,double.sample(.05,.25))
count = sum(rs<=.05)
count / 10000
```

Is there an inflation of false positives? How bad is it?

<<<<<<< HEAD
This is already pretty bad, we're gaining about 50% of our original false positive rate. (Attempt one was .0716, two .07), looks like on average it comes out around .07.

=======
>>>>>>> 8dce80c003f0c712a34dea938a876fd5107fb469
Now modify this code so that you can investigate this "double the sample" rule in a bit more depth. Let's see what happens when you double the sample ANY time p > .05 (not just when p < .25), or when you do it only if p < .5 or < .75. How do these choices affect the false positive rate?

HINT: Try to do this by making the function `double.sample` take the upper p value as an argument, so that you can pass this through dplyr.

HINT 2: You may need more samples. Find out by looking at how the results change from run to run.

<<<<<<< HEAD

```{r}
rs2 = replicate(10000,double.sample(.05,1))
c2 = sum(rs2<=.05)
c2 / 10000
```

Anything above p>.05 is really bad, we're getting close to doubling our false positive rate at this point.


```{r}
rs2 = replicate(10000,double.sample(.05,.75))
c2 = sum(rs2<=.05)
c2 / 10000
```

.0817, .0772.

The damage of this policy directly depends on how large the allowable range is. We can go farther for example and look at what happens if we restrict ourselves to repeating at lower p-values as well.

```{r}
rs2 = replicate(50000,double.sample(.01,.25))
c2 = sum(rs2<=.01)
c2 / 50000
```

This is pretty bad for NHST, we need to be very careful about designs to ensure that this isn't affecting results especially when they are on the borderline. Again, it's another suggestion that we shouldn't be doing NHST--we should be reporting effect sizes and considering them in a theoretical framework.


Part 2: The Linear Model
========================

2A: Basic Linear Modeling
-------------------------

Let's use the `ToothGrowth` dataset, on guineapig teeth based on orange juice
and vitamin C. This is super simple. (Try `?ToothGrowth`).

First plot the data, we'll use `qplot` to understand how `len` (tooth length) depends on `dose` (amount of Vitamin C) and `supp` (delivery method).

```{r}
ggplot(ToothGrowth,aes(dose,len,color=supp)) +
  geom_point(position=position_jitter(width=.05,height=0)) +
  geom_smooth(method='lm')

```

So now you see what's going on. 

Next, always make a histogram of the DV before making a linear model! This reveals the distribution and can be helpful in choosing your model type.

```{r}
ggplot(ToothGrowth) +
  geom_histogram(aes(x=len)) +
  facet_grid(.~supp)
```

This doesn't look normal, but we should keep in mind what we saw in the overall plot--the differenc emay be due to a bimodal distribution at lower doses.

Now make a linear model of tooth lengths using `lm`. Try making one with main effects and interactions and another with just main  effects. Make sure to assign them to variables so that you can get them later.

```{r}
rs1 = lm(data=ToothGrowth,len~supp+dose)
rs2 = lm(data=ToothGrowth,len~supp*dose)
```

Now make a linear model of tooth lengths using `lm`. Try making one with main effects and interactions and another with just main  effects. Make sure to assign them to variables so that you can get them later.


Now try taking out the intercept, using a -1 term in the formula. what does this do?

```{r}
rs1_noi = lm(data=ToothGrowth,len~supp+dose-1)
```

The model is now len ~ dose + supp + e instead of len ~ i + dose + supp + e, dose being 0.5, 1.0, 2.0 and supp being a factor with two levels. 

Thought question: Take a moment to interpret the coefficients of the model. 
Q1 - What are the units?
Great question, probably mm. Doesn't specify in the dataset.
Q2 - How does the interaction relate to the plot?
It allows for the different slopes (across doses) for the different supp groups.
Q3 - Should there be an interaction in the model? What does it mean? How important is it?
If we trust the plot then yes, probably. There is very different tooth growth at low doses (0.5) while the difference vanishes at high doses (2.0). The interaction seems to be important for low doses but not at high ones.

Now make predictions from the model you like the best. What should happen with
doses of 0, 1.5, 2.5, and 10 under both supplements? 

I'll use rs2 since an anova across results suggests that, despite the increase in degrees of freedom with the interaction term we nevertheless capture significantly more variance.

```{r}
anova(rs1,rs2)
```

HINT: use the `predict` function ...

HINT 2: you will have to make a dataframe to do the prediction with, so use something like `data.frame(dose=...)`.

```{r}
predictAt = data.frame(dose=c(0,1.5,2.5,10,0,1.5,2.5,10),supp=c("OJ","OJ","OJ","OJ","VC","VC","VC","VC"))
out = predict(rs1,predictAt)
```

Now plot the residuals from the original model. How do they look?
HINT: `?resid`

```{r}
plot(resid(rs1))
```


BONUS: test them for normality of distribution using a quantile-quantile plot.

HINT: `?qqplot` and `?qqnorm`

```{r}
qqnorm(resid(rs1))
```


2B: Exploratory Linear Modeling
-------------------------------

What the heck is going on? Load data from Frank, Vul, Saxe (2011, Infancy), a study in which we measured infants' looking to hands in moving scenes. There were infants from 3 months all the way to about two years, and there were two movie conditions (`Faces_Medium`, in which kids played on a white background, and `Faces_Plus`, in which the backgrounds were more complex and the people in the videos were both kids and adults). Forgive our bad naming conventions.

Try to figure out what the most reasonable linear model of the data is.

```{r}
d <- read.csv("C:\\Users\\dan\\proj\\repo254\\data\\FVS2011-hands.csv")
library(lme4)
```

```{r}
rs1 = lm(data=d,hand.look~1)
rs2 = lm(data=d,hand.look~age)
rs3 = lm(data=d,hand.look~age+condition)
rs4 = lm(data=d,hand.look~age*condition)
anova(rs1,rs2,rs3,rs4)
```

```{r}
rs5= lmer(data=d, hand.look ~ age * condition + (1|subid))
AIC(rs4)
AIC(rs5)
```

According to a stepwise forward regression we identified the best predictive model to be a linear model with age, condition, and their interaction, but no random effect of individual intercept.

Plot that model on the same plot as the data.

HINT: you can do this either using `predict` or (if you are feeling confident of your understanding of the models) using the built-in linear models in `ggplot`'s `geom_smooth`. 

```{r}
d %>%
  ggplot(data=.,aes(age,hand.look,color=condition)) +
  geom_point() + 
  geom_smooth(method='lm',formula=y~x)
```

What do you conclude from this pattern of data?

We computed a linear regression model predicting hand looking time (secs) from age (months), and condition (kids alone on a white background or kids and adults in a complex scene). We found that looking time increased (4 ms/mo) more quickly for the complex scenes than the less complex ones, $\beta_{age*condition}$ = .004, t(228) = 2.39, p = .018$. In addition, looking time increases as age increases at a rate of approx. 3 ms / mo, $\beta_{age} = .003, t(228) = 2.67, p = .008$.

What do you conclude from this pattern of data?

3: Linear Mixed Effect Models
=============================

The goal here is to learn to use LMEMs using `lme4` and to compare them to
standard by subject, by item LMs, as well as the standard (no repeated measures) fixed effects GLM.

The dataset here is from Stiller, Goodman, & Frank (2014), a paper on children's pragmatic inferences. We saw the paradigm in the counterbalancing lecture: it's three faces: a smiley, a smiley with glasses, and a smiley with a hat and glasses. When told "my friend has glasses" do kids pick the one with the glasses and no hat? `age.group` is the kids' age group, `condition` is either "label," described above, or "no label," which was a control condition in which kids picked without hearing the term "glasses" at all. 

```{r}
d <- read.csv("C:\\Users\\dan\\proj\\repo254\\data\\scales.csv")
d$age.group <- factor(d$age.group)
```

Always begin with a histogram!

```{r}
# Not a histogram, but I find this more useful for looking at the data.
d %>%
  group_by(age,condition) %>%
  summarise(mu=mean(correct)) %>%
  ggplot(.,aes(age,mu,color=condition)) +
  geom_point(size=3) +
  ylab('% Correct') +
  theme_bw()
```

Brief Confidence Interval Digression
------------------------------------

Start out by setting up a function for a 95% CI using the normal approximation.

```{r}
ci95 <- function(x) {
  mu = mean(x)
  s = sd(x)
  if (length(x) >= 30) {
    return(c(mu-1.96*s,mu+1.96*s))
  } else {
    val = qt(.975,length(x))
    return(c(mu-val*s,mu+val*s))
  }
}
```

But the number of participants in a group is likely to be < 30, so let's also compute this with a t distribution.

```{r}
# I automated this with the above function
```

On the other hand, maybe we should use bootstrap CIs because these are actually  proportions, and the normal/t approximations don't know that they are 0/1 bounded.

```{r}
library(boot)
library(bootstrap)
```

Take a look at `?boot` and `?bootci`. Note that the syntax for the `boot` library is terrible, so we're going to use it to check some code that I use:

```{r}
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}
ci.low <- function(x,na.rm=T) {
  mean(x,na.rm=na.rm) - 
    quantile(bootstrap(1:length(x),
                       10000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)}
ci.high <- function(x,na.rm=T) {
  quantile(bootstrap(1:length(x),
                     10000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) - 
    mean(x,na.rm=na.rm)}
ci.boot <- function(x) {
  return(c(ci.low(x),ci.high(x)))
}
```

Now make 95% confidence intervals across participants using all the methods above:

- Normal
- t
- Bootstrap percentile using `boot.ci`
- Bootstrap percentile using my code

```{r}
ci95(d$correct) # note this does the t or normal, whichever is appropriate
samplemean <- function(x,d) {return(mean(x[d]))}
boot.ci(boot(d$correct,samplemean,1000))
ci.boot(d$correct)
```

Now plot the data by age and condition using `dplyr` and `ggplot2`. Plot some CIs on here - extra credit if you plot all of them and compare visually (you'll need `position = position_dodge()` or some other way to offset them).  

```{r}
d %>%
  group_by(age.group,condition) %>%
  summarise(mu=mean(correct),high=ci.high(correct),low=ci.low(correct)) %>%
  ggplot(.,aes(age.group,mu,fill=condition)) +
  geom_bar(stat="identity",position=position_dodge(),width=.6) +
  geom_errorbar(aes(age.group,mu,ymin=mu-low,ymax=mu+high),width=0,position=position_dodge(width=.6)) +
  ylab("Mean % Correct") +
  xlab("Age Group") +
  theme_bw()

```

What do you conclude about confidence interval computation?

Not sure what the point is here.. I prefer bootstrapping in my own statistics because it preserves the true sampling distribution... not much more to say. Unless bootstrapping is computationally infeasible it should be used.

Back to LMEMs
-------------

```{r}
library(lme4)
```

OK, now do a basic GLM over the entire data frame, using `age.group`, `condition`, and their interaction to predict correctness. (If we were focusing on developmental issues, I would ask you to think about how to model age here, but let's treat it as three discrete groups for now). 

NOTE: this model is not appropriate, because it assumes that each subject's observations are independent from one another. It's still fine to do the analysis, though: it can tell you a lot about the data and is easy and fast to fit, as long as you know that you can't trust the p-values!

```{r}
# since we're just using this as a test, I'll pretend the age groups are numeric so we can get an easy to interpret estimate of the interaction
rs = glm(data=d,correct ~ as.numeric(age.group) * condition)
summary(rs)
```

Let's now use `dplyr` to get data frames for by-items (`msi`) and by-subjects (`mss`) analyses. `msi` should contain the mean ratings for every item and `mss` should contain the mean ratings for every subject.

```{r}
msi = d %>%
  group_by(trial,age.group,condition) %>%
  summarise(mu=mean(correct))
mss = d %>%
  group_by(subid,age.group,condition) %>%
  summarise(mu=mean(correct))
```

Now do standard linear models on each of these.

NOTE: These are not strictly correct either because of the normal approximation on percent correct (model doesn't know it's 0 - 1 bounded and could give you standard error that goes above 1). Again, useful to do and see what happens.

```{r}
rs1 = glm(data=msi,mu~as.numeric(age.group) * condition)
rs2 = glm(data=mss,mu~as.numeric(age.group) * condition)
```
There's basically no difference between these models

Do ANOVA on these. Note that ANOVA doesn't let you figure out what is going on with individual levels of age.

```{r}
# I can't use the anova, so I just compared AIC
AIC(rs1) # better model according to AIC
AIC(rs2)
# anova(rs1,rs2)
```

The AIC outcome suggests the rs1 model fits better, so the data have more variability explained by items than by subjects.

On to linear mixed effect models. Create the maximal random effects model a la Barr et al. (2013). Does it converge? If not, what will you do to make it converge? (The internet can be your friend here).

HINT: try simplifying your model to a "semi-maximal" model. Bonus: try using a different fitting procedure on the maximal model.

HINT: make sure that you consider which random effects are appropriate. Consider which observations are within/between subjects. E.g. having a random coefficient for age by subject doesn't make sense, because each subject has only one age.


```{r}
# Ian helped me figure out that using this optimizer would allow the model to converge, although I also noticed that the correlation between a lot of the random effects is 1--suggesting that something is very wrong.
rs = glmer(data=d,correct ~ as.numeric(age.group)*condition + (condition*age.group|trial) + (1|subid),family="binomial",control=glmerControl(optimizer="bobyqa"))

# rather than use this, I'll simplify more... so that w eodn't need to the optimizer
rs = glmer(data=d,correct ~ as.numeric(age.group)*condition + (condition+as.numeric(age.group)|trial) + (1|subid),family="binomial")
```

How do these coefficients compare with the independent coefficients linear model? What do you conclude?
Fixed effects:
                                        Estimate Std. Error z value Pr(>|z|)    
(Intercept)                              -0.1652     0.3660  -0.451 0.651726    
as.numeric(age.group)                     0.5762     0.2014   2.861 0.004218 ** 
conditionNo Label                        -0.4865     0.5509  -0.883 0.377223    
as.numeric(age.group):conditionNo Label  -0.9902     0.2795  -3.542 0.000397 ***

From lm:
                                        Estimate Std. Error t value Pr(>|t|)    
(Intercept)                              0.48326    0.06608   7.314 8.62e-13 ***
as.numeric(age.group)                    0.10071    0.03081   3.269 0.001144 ** 
conditionNo Label                       -0.16729    0.09427  -1.775 0.076491 .  
as.numeric(age.group):conditionNo Label -0.15800    0.04379  -3.608 0.000335 ***

The main changes are in the intercept, where there is now much more variance allotted to the model intercept. This redistribution likely occurred because a large portion of variance i


Which random effects make the most difference? Find out using `ranef`. Plot the random effects for subject and item.

Some subjects have very high variability, but the items, in particular the faces and houses, seem to have a very high within-item variability.

```{r}
ranef(rs)
```

Make the minimal random effects model with just a subject intecept. How does this compare?

```{r}
rsmin = glmer(data=d,correct ~ (1|subid),family="binomial")
```

I really don't know how this is supposed to be comparable... now we just have the intercept and the onerandom effect. 

Get an estimate of the significance value for the coefficient on the `age*condition` interaction by using anova to compare between your semi-maximal model and the model without an intercept.

What?? I have no idea why this anova would do that...
```{r}
anova(rs,rsmin)

rsmax_noint = glmer(data=d,correct ~ as.numeric(age.group)+condition + (condition+as.numeric(age.group)|trial) + (1|subid),family="binomial")
anova(rsmax_noint,rs)
```

I mean I guess this suggests that everything we added is important for the model, so it's clear that the age*condition interaction is important compared to the minimal model, but we added a lot of other things as well. Better to do the comparison with the age+condition mode (the second anova)... that one does show that the interaction is clearly important for the model, reducing the AIC significantly despite the increased degrees of fredom.
Seems like there's a typo in the hw, but since the class is basically over and the project is handed in and I know how to do mixed effect modeling in my fMRI data I'm not going to try and figure this outany further. 

Thanks for everything! The project portion of the class was wonderful!
